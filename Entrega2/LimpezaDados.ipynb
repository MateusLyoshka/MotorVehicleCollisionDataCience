{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51eb4a98",
   "metadata": {},
   "source": [
    "# Importação e Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "import duckdb\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import ceil\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Motor_Vehicle_Collisions_-_Crashes_20251015.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0e613",
   "metadata": {},
   "source": [
    "## Verificando nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum() # Muitos valores nulos impossivel exclui-los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # colunas off street name, contributing factor 3 veicle ate o 5 , vehicle type code 3 ate o 5 sao colunas com muitos valores nulos\n",
    "missing_percent = df.isna().mean() * 100\n",
    "print(missing_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4b9fa",
   "metadata": {},
   "source": [
    "# Lidando Com Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df779ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os outliars de longitude e latitude em Nan\n",
    "# lat_min, lat_max = 40.49, 40.92\n",
    "# lon_min, lon_max = -74.26, -73.69\n",
    "# filtro_invalidas = (\n",
    "#     (df['LATITUDE'] < lat_min) | (df['LATITUDE'] > lat_max) |\n",
    "#     (df['LONGITUDE'] < lon_min) | (df['LONGITUDE'] > lon_max)\n",
    "# )\n",
    "# df.loc[filtro_invalidas, ['LATITUDE', 'LONGITUDE']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557262f",
   "metadata": {},
   "source": [
    "## Lidando Com Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e68631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f943c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Padronização de Texto e Imputação de Coordenadas\")\n",
    "\n",
    "# 1. Padronização de Texto\n",
    "text_cols = ['ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME', 'BOROUGH']\n",
    "\n",
    "print(\"Padronizando nomes de ruas para maiúsculo...\")\n",
    "for col in text_cols:\n",
    "    # Converte para upper e remove espaços\n",
    "    df[col] = df[col].astype(str).str.upper().str.strip()\n",
    "    df[col] = df[col].replace({'NAN': np.nan, 'NONE': np.nan})\n",
    "\n",
    "missing_start = df['LATITUDE'].isna().sum()\n",
    "print(f\"\\nNulos iniciais em LATITUDE: {missing_start:,}\")\n",
    "\n",
    "# PASSO 1: Interseção (Rua A & Rua B)\n",
    "mask_intersection = (df['ON STREET NAME'].notna()) & (df['CROSS STREET NAME'].notna())\n",
    "df.loc[mask_intersection, 'intersection_key'] = (\n",
    "    df.loc[mask_intersection, 'ON STREET NAME'] + \" & \" + \n",
    "    df.loc[mask_intersection, 'CROSS STREET NAME']\n",
    ")\n",
    "\n",
    "print(\"1. Tentando imputar por Interseção...\")\n",
    "for col in ['LATITUDE', 'LONGITUDE']:\n",
    "    df[col] = df[col].fillna(df.groupby('intersection_key')[col].transform('mean'))\n",
    "\n",
    "# PASSO 2: Ruas Individuais\n",
    "street_cols = ['ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME']\n",
    "for street_col in street_cols:\n",
    "    print(f\"2. Tentando imputar por '{street_col}'...\")\n",
    "    for col in ['LATITUDE', 'LONGITUDE']:\n",
    "        df[col] = df[col].fillna(df.groupby(street_col)[col].transform('mean'))\n",
    "\n",
    "# PASSO 3: ZIP CODE\n",
    "if 'ZIP CODE' in df.columns:\n",
    "    print(\"3. Tentando imputar por ZIP CODE...\")\n",
    "    for col in ['LATITUDE', 'LONGITUDE']:\n",
    "        df[col] = df[col].fillna(df.groupby('ZIP CODE')[col].transform('mean'))\n",
    "\n",
    "# Limpeza final\n",
    "if 'intersection_key' in df.columns:\n",
    "    df = df.drop(columns=['intersection_key'])\n",
    "\n",
    "missing_end = df['LATITUDE'].isna().sum()\n",
    "recovered = missing_start - missing_end\n",
    "\n",
    "print(f\"Total recuperado: {recovered:,} ({recovered/missing_start:.1%} dos nulos)\")\n",
    "print(f\"Restantes sem coordenada: {missing_end:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica o padrão dos que sobraram sem Latitude\n",
    "missing_coords = df[df['LATITUDE'].isna()]\n",
    "\n",
    "print(\"Ruas nos dados faltantes:\")\n",
    "print(missing_coords['ON STREET NAME'].value_counts().head())\n",
    "\n",
    "print(\"\\nZIP CODES nos dados faltantes:\")\n",
    "print(missing_coords['ZIP CODE'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Limpeza Final e Filtragem Geográfica\")\n",
    "\n",
    "# 1. Criação do df_cleaned removendo linhas sem coordenada\n",
    "df_cleaned = df.dropna(subset=['LATITUDE', 'LONGITUDE']).copy()\n",
    "\n",
    "# 2. Remove pontos que caíram fora\n",
    "lat_min, lat_max = 40.49, 40.92\n",
    "lon_min, lon_max = -74.26, -73.69\n",
    "\n",
    "# Máscara para manter apenas o que está DENTRO de NYC\n",
    "mask_nyc = (\n",
    "    (df_cleaned['LATITUDE'] >= lat_min) & (df_cleaned['LATITUDE'] <= lat_max) &\n",
    "    (df_cleaned['LONGITUDE'] >= lon_min) & (df_cleaned['LONGITUDE'] <= lon_max)\n",
    ")\n",
    "\n",
    "outliers = (~mask_nyc).sum()\n",
    "df_cleaned = df_cleaned[mask_nyc].copy()\n",
    "\n",
    "print(f\"Outliers geográficos removidos: {outliers:,}\")\n",
    "print(f\"Total final (df_cleaned): {len(df_cleaned):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(\"Imputação de bairros via Coordenadas (KNN)\")\n",
    "\n",
    "# 1. Diagnóstico Inicial\n",
    "missing_borough_start = df['BOROUGH'].isna().sum()\n",
    "print(f\"Nulos iniciais em BOROUGH: {missing_borough_start:,}\")\n",
    "\n",
    "# 2. Preparação dos Dados para o KNN\n",
    "mask_train = df['BOROUGH'].notna() & df['LATITUDE'].notna() & df['LONGITUDE'].notna()\n",
    "mask_target = df['BOROUGH'].isna() & df['LATITUDE'].notna() & df['LONGITUDE'].notna()\n",
    "\n",
    "if mask_target.sum() > 0:\n",
    "    print(f\"Treinando KNN com {mask_train.sum():,} registros...\")\n",
    "    \n",
    "    # Classificador de 1 vizinho mais próximo \n",
    "    knn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)\n",
    "    \n",
    "    # Treino\n",
    "    X_train = df.loc[mask_train, ['LATITUDE', 'LONGITUDE']]\n",
    "    y_train = df.loc[mask_train, 'BOROUGH']\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Prevendo bairros para {mask_target.sum():,} registros...\")\n",
    "    \n",
    "    # Prevê os bairros faltantes\n",
    "    X_target = df.loc[mask_target, ['LATITUDE', 'LONGITUDE']]\n",
    "    predicted_boroughs = knn.predict(X_target)\n",
    "    \n",
    "    # Preenche os nulos no DataFrame original\n",
    "    df.loc[mask_target, 'BOROUGH'] = predicted_boroughs\n",
    "\n",
    "# 3. Resultado Final\n",
    "missing_borough_end = df['BOROUGH'].isna().sum()\n",
    "recovered = missing_borough_start - missing_borough_end\n",
    "\n",
    "print(f\"Bairros recuperados: {recovered:,}\")\n",
    "print(f\"Ainda sem bairro (porque não tinham coordenada): {missing_borough_end:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preenchendo Colunas de Veículo com 'N/A'\")\n",
    "\n",
    "# Seleção explícita das colunas\n",
    "vehicle_cols = [\n",
    "    'VEHICLE TYPE CODE 1', \n",
    "    'VEHICLE TYPE CODE 2', \n",
    "    'VEHICLE TYPE CODE 3', \n",
    "    'VEHICLE TYPE CODE 4', \n",
    "    'VEHICLE TYPE CODE 5'\n",
    "]\n",
    "\n",
    "# 1. Preenche valores nulos reais (NaN) com 'N/A'\n",
    "df[vehicle_cols] = df[vehicle_cols].fillna('N/A')\n",
    "\n",
    "# 2. Garante que tudo é texto e remove 'nan' textual se existir\n",
    "df[vehicle_cols] = df[vehicle_cols].astype(str).replace(\n",
    "    {'nan': 'N/A', 'NaN': 'N/A', '<NA>': 'N/A', 'None': 'N/A'}\n",
    ")\n",
    "\n",
    "print(\"\\nVerificação (VEHICLE TYPE CODE 1):\")\n",
    "print(df['VEHICLE TYPE CODE 1'].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1122710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preenchimento de Nulos\")\n",
    "\n",
    "# 1. Vítimas: Preencher com 0\n",
    "victim_cols = ['NUMBER OF PERSONS KILLED', 'NUMBER OF PERSONS INJURED']\n",
    "for col in victim_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "# 2. Causas: Preencher com 'Unspecified'\n",
    "factor_cols = [\n",
    "    'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 5'\n",
    "]\n",
    "df[factor_cols] = df[factor_cols].fillna('Unspecified')\n",
    "\n",
    "# 3. Ruas e CEP: Preencher com 'N/A'\n",
    "text_cols = ['ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME', 'ZIP CODE']\n",
    "df[text_cols] = df[text_cols].fillna('N/A')\n",
    "\n",
    "# 4. Location: Redundante\n",
    "if 'LOCATION' in df.columns:\n",
    "    df = df.drop(columns=['LOCATION'])\n",
    "\n",
    "print(\"Nulos restantes por coluna:\")\n",
    "print(df.isna().sum()[df.isna().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remoção final\")\n",
    "\n",
    "total_antes = len(df)\n",
    "\n",
    "# 1. Deleta quem não tem latitude ou longitude\n",
    "df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# 2. Verificação Final\n",
    "total_depois = len(df)\n",
    "removidos = total_antes - total_depois\n",
    "\n",
    "print(f\"Linhas removidas: {removidos:,} ({(removidos/total_antes):.1%} do total)\")\n",
    "print(f\"Total de linhas restantes: {total_depois:,}\")\n",
    "df_imputed = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3119125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "missing_percent = df.isna().mean() * 100\n",
    "print(missing_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92beece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1946165",
   "metadata": {},
   "source": [
    "# Resolvendo inconsistências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica colunas do tipo texto (object)\n",
    "object_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in object_cols:\n",
    "    num_unique = df_imputed[col].nunique()\n",
    "    print(f\"\\nColuna: '{col}' | Valores Únicos: {num_unique}\")\n",
    "\n",
    "    # Exibe contagem se for baixa cardinalidade, caso contrário mostra uma amostra\n",
    "    if num_unique <= 50:\n",
    "        print(df_imputed[col].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(f\"Amostra: {df_imputed[col].sample(5, random_state=1).to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4256bb",
   "metadata": {},
   "source": [
    "# Resolvendo Tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typed = df_imputed.copy()\n",
    "\n",
    "# Convertendo colunas que comecam com \"NUMBER\" para int\n",
    "number_cols = [col for col in df_typed.columns if col.startswith(\"NUMBER\")]\n",
    "for col in number_cols:\n",
    "    df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7565da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizado colunas de datas\n",
    "date_cols = [col for col in df_typed.columns if 'DATE' in col.upper() or 'TIME' in col.upper()]\n",
    "print(\"Date/Time columns found:\")\n",
    "print(date_cols)\n",
    "print(\"\\nSample values:\")\n",
    "for col in date_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df_typed[col].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3222bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversão de tipos para data e hora\n",
    "df_typed['CRASH DATE'] = pd.to_datetime(df_typed['CRASH DATE'], format='%m/%d/%Y', errors='coerce')\n",
    "df_typed['CRASH TIME'] = pd.to_datetime(df_typed['CRASH TIME'], format='%H:%M', errors='coerce').dt.time\n",
    "\n",
    "# Criação da coluna de dia da semana\n",
    "df_typed['DAY_OF_WEEK'] = df_typed['CRASH DATE'].dt.day_name()\n",
    "\n",
    "# Visualização do resultado\n",
    "print(df_typed[['CRASH DATE', 'CRASH TIME', 'DAY_OF_WEEK']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c583966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona colunas 'object', excluindo 'CRASH TIME' (que é datetime.time)\n",
    "object_cols = df_typed.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'CRASH TIME' in object_cols:\n",
    "    object_cols.remove('CRASH TIME')\n",
    "\n",
    "# Converte as colunas selecionadas para o tipo 'string' de uma só vez\n",
    "df_typed[object_cols] = df_typed[object_cols].astype('string')\n",
    "\n",
    "print(f\"{len(object_cols)} colunas convertidas para o tipo string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#versao sem ID e dropando location ja que e a mesma coisa de lat long, alem de renomear o dataset final\n",
    "df_noid = df_typed.drop(columns=['COLLISION_ID'])\n",
    "print(df_noid)\n",
    "df_noid.info()\n",
    "df_cleaned = df_typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ece866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset final limpo\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af8d3e",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dados originais para comparação\n",
    "df_raw = pd.read_csv(\"../Motor_Vehicle_Collisions_-_Crashes_20251015.csv\")\n",
    "\n",
    "# Bounding box NYC\n",
    "lat_min, lat_max = 40.49, 40.92\n",
    "lon_min, lon_max = -74.26, -73.69\n",
    "\n",
    "# Identifica registros fora dos limites geográficos no dataset original\n",
    "raw_outlier_mask = (\n",
    "    (df_raw['LATITUDE'] < lat_min) | (df_raw['LATITUDE'] > lat_max) |\n",
    "    (df_raw['LONGITUDE'] < lon_min) | (df_raw['LONGITUDE'] > lon_max)\n",
    ")\n",
    "\n",
    "print(f\"Comparativo de Registros: Raw ({len(df_raw)}) vs Cleaned ({len(df_cleaned)})\")\n",
    "print(f\"Outliers de coordenadas detectados no Raw: {raw_outlier_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"LATITUDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar conexão DuckDB em memória se tiver pouca RAM o bagui fica feio\n",
    "\n",
    "conn = duckdb.connect(':memory:')\n",
    "\n",
    "# Registrar no DuckDB\n",
    "conn.register('crashes_raw', df_raw) \n",
    "conn.register('crashes_clean', df_cleaned)\n",
    "\n",
    "print('Tabelas criadas: crashes_raw, crashes_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {\n",
    "    'count_rows_raw': \"SELECT COUNT(*) AS total_raw FROM crashes_raw;\",\n",
    "    'count_rows_clean': \"SELECT COUNT(*) AS total_clean FROM crashes_clean;\",\n",
    "\n",
    "    # Queries para contagem de nulos\n",
    "    'nulls_raw': \"\"\"\n",
    "        SELECT column_name, SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) AS nulls,\n",
    "               COUNT(*) AS total, ROUND(100.0*SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END)/COUNT(*),2) AS pct_null\n",
    "        FROM (\n",
    "          SELECT CAST(\"CRASH DATE\" AS VARCHAR) AS value, 'CRASH DATE' AS column_name FROM crashes_raw UNION ALL\n",
    "          SELECT CAST(\"CRASH TIME\" AS VARCHAR), 'CRASH TIME' FROM crashes_raw UNION ALL\n",
    "          SELECT CAST(LATITUDE AS VARCHAR), 'LATITUDE' FROM crashes_raw UNION ALL\n",
    "          SELECT CAST(LONGITUDE AS VARCHAR), 'LONGITUDE' FROM crashes_raw UNION ALL\n",
    "          SELECT CAST(\"NUMBER OF PERSONS INJURED\" AS VARCHAR), 'NUMBER OF PERSONS INJURED' FROM crashes_raw UNION ALL\n",
    "          SELECT CAST(\"NUMBER OF PERSONS KILLED\" AS VARCHAR), 'NUMBER OF PERSONS KILLED' FROM crashes_raw UNION ALL\n",
    "          SELECT \"CONTRIBUTING FACTOR VEHICLE 1\", 'CONTRIBUTING FACTOR VEHICLE 1' FROM crashes_raw UNION ALL\n",
    "          SELECT \"CONTRIBUTING FACTOR VEHICLE 2\", 'CONTRIBUTING FACTOR VEHICLE 2' FROM crashes_raw UNION ALL\n",
    "          SELECT \"VEHICLE TYPE CODE 1\", 'VEHICLE TYPE CODE 1' FROM crashes_raw UNION ALL\n",
    "          SELECT \"VEHICLE TYPE CODE 2\", 'VEHICLE TYPE CODE 2' FROM crashes_raw\n",
    "        ) t GROUP BY column_name ORDER BY pct_null DESC;\"\"\",\n",
    "    \n",
    "    'nulls_clean': \"\"\"\n",
    "        SELECT column_name, SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) AS nulls,\n",
    "               COUNT(*) AS total, ROUND(100.0*SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END)/COUNT(*),2) AS pct_null\n",
    "        FROM (\n",
    "          SELECT CAST(\"CRASH DATE\" AS VARCHAR) AS value, 'CRASH DATE' AS column_name FROM crashes_clean UNION ALL\n",
    "          SELECT CAST(\"CRASH TIME\" AS VARCHAR), 'CRASH TIME' FROM crashes_clean UNION ALL\n",
    "          SELECT CAST(LATITUDE AS VARCHAR), 'LATITUDE' FROM crashes_clean UNION ALL\n",
    "          SELECT CAST(LONGITUDE AS VARCHAR), 'LONGITUDE' FROM crashes_clean UNION ALL\n",
    "          SELECT CAST(\"NUMBER OF PERSONS INJURED\" AS VARCHAR), 'NUMBER OF PERSONS INJURED' FROM crashes_clean UNION ALL\n",
    "          SELECT CAST(\"NUMBER OF PERSONS KILLED\" AS VARCHAR), 'NUMBER OF PERSONS KILLED' FROM crashes_clean UNION ALL\n",
    "          SELECT \"CONTRIBUTING FACTOR VEHICLE 1\", 'CONTRIBUTING FACTOR VEHICLE 1' FROM crashes_clean UNION ALL\n",
    "          SELECT \"CONTRIBUTING FACTOR VEHICLE 2\", 'CONTRIBUTING FACTOR VEHICLE 2' FROM crashes_clean UNION ALL\n",
    "          SELECT \"VEHICLE TYPE CODE 1\", 'VEHICLE TYPE CODE 1' FROM crashes_clean UNION ALL\n",
    "          SELECT \"VEHICLE TYPE CODE 2\", 'VEHICLE TYPE CODE 2' FROM crashes_clean\n",
    "        ) t GROUP BY column_name ORDER BY pct_null DESC;\"\"\",\n",
    "\n",
    "    # Top Fatores e Veículos\n",
    "    'top_factors_raw': \"\"\"SELECT \"CONTRIBUTING FACTOR VEHICLE 1\" AS f, COUNT(*) AS c FROM crashes_raw WHERE \"CONTRIBUTING FACTOR VEHICLE 1\" IS NOT NULL AND TRIM(\"CONTRIBUTING FACTOR VEHICLE 1\") <> '' GROUP BY f ORDER BY c DESC LIMIT 15;\"\"\",\n",
    "    'top_factors_clean': \"\"\"SELECT \"CONTRIBUTING FACTOR VEHICLE 1\" AS f, COUNT(*) AS c FROM crashes_clean WHERE \"CONTRIBUTING FACTOR VEHICLE 1\" IS NOT NULL AND TRIM(\"CONTRIBUTING FACTOR VEHICLE 1\") <> '' GROUP BY f ORDER BY c DESC LIMIT 15;\"\"\",\n",
    "    \n",
    "    'vehicle_types_raw': \"\"\"SELECT \"VEHICLE TYPE CODE 1\" AS v, COUNT(*) AS c FROM crashes_raw WHERE \"VEHICLE TYPE CODE 1\" IS NOT NULL AND TRIM(\"VEHICLE TYPE CODE 1\") <> '' GROUP BY v ORDER BY c DESC LIMIT 15;\"\"\",\n",
    "    'vehicle_types_clean': \"\"\"SELECT \"VEHICLE TYPE CODE 1\" AS v, COUNT(*) AS c FROM crashes_clean WHERE \"VEHICLE TYPE CODE 1\" IS NOT NULL AND TRIM(\"VEHICLE TYPE CODE 1\") <> '' GROUP BY v ORDER BY c DESC LIMIT 15;\"\"\",\n",
    "\n",
    "    # Agrupamentos Mensais\n",
    "    'monthly_raw': \"\"\"SELECT SUBSTRING(\"CRASH DATE\", 7, 4) || '-' || SUBSTRING(\"CRASH DATE\", 1, 2) AS ym, COUNT(*) AS cnt FROM crashes_raw WHERE \"CRASH DATE\" IS NOT NULL GROUP BY ym ORDER BY ym;\"\"\",\n",
    "    'monthly_clean': \"\"\"SELECT DATE_TRUNC('month', \"CRASH DATE\")::VARCHAR AS ym, COUNT(*) AS cnt FROM crashes_clean WHERE \"CRASH DATE\" IS NOT NULL GROUP BY ym ORDER BY ym;\"\"\"\n",
    "}\n",
    "\n",
    "# Execução e armazenamento dos resultados\n",
    "results = {}\n",
    "for name, q in queries.items():\n",
    "    results[name] = conn.execute(q).df()\n",
    "    print(f\"-- {name}\\n{results[name].head()}\\n\")\n",
    "\n",
    "# Variáveis para uso posterior\n",
    "monthly_raw = results['monthly_raw']\n",
    "monthly_clean = results['monthly_clean']\n",
    "nulls_raw = results['nulls_raw']\n",
    "nulls_clean = results['nulls_clean']\n",
    "veh_raw = results['vehicle_types_raw']\n",
    "veh_clean = results['vehicle_types_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03141e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#1. Raw vs Clean\n",
    "miss_compare = nulls_raw[['column_name','pct_null']].merge(\n",
    "    nulls_clean[['column_name','pct_null']], on='column_name', suffixes=('_raw','_clean')\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=miss_compare.melt(id_vars='column_name', value_name='pct_null', var_name='dataset'), \n",
    "            x='column_name', y='pct_null', hue='dataset')\n",
    "plt.xticks(rotation=45, ha='right'); plt.title('Percentual de Nulos: Raw vs Clean'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2. Evolução Mensal \n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(monthly_raw['ym'], monthly_raw['cnt'], label='Raw', alpha=0.6)\n",
    "plt.plot(monthly_clean['ym'], monthly_clean['cnt'], label='Clean', alpha=0.6)\n",
    "plt.xticks(monthly_clean['ym'][::max(len(monthly_clean)//12, 1)], rotation=45)\n",
    "plt.title('Contagem Mensal de Colisões'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3. Top Tipos de Veículo \n",
    "veh_raw = veh_raw.rename(columns={'v': 'vtype', 'c': 'cnt'})\n",
    "veh_clean = veh_clean.rename(columns={'v': 'vtype', 'c': 'cnt'})\n",
    "\n",
    "veh_merge = veh_raw.merge(veh_clean, on='vtype', suffixes=('_raw','_clean'))\n",
    "veh_sorted = veh_merge.sort_values('cnt_clean', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=veh_sorted.melt(id_vars='vtype', value_vars=['cnt_raw','cnt_clean']), \n",
    "            x='vtype', y='value', hue='variable')\n",
    "plt.xticks(rotation=45, ha='right'); plt.title('Top 15 Veículos: Raw vs Clean'); plt.ylabel('Contagem'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4. Coordenadas Geográficas \n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].scatter(df_raw['LONGITUDE'], df_raw['LATITUDE'], s=1, alpha=0.3)\n",
    "axes[0].set_title(f'Raw: Com Erros de GPS (n={len(df_raw):,})')\n",
    "axes[0].set_xlim(-80, -70) # Zoom out\n",
    "axes[0].set_ylim(35, 45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(df_cleaned['LONGITUDE'], df_cleaned['LATITUDE'], \n",
    "                s=0.1, alpha=0.1, color='#003366') \n",
    "axes[1].set_title(f'Clean: Mapa de NYC (n={len(df_cleaned):,})')\n",
    "\n",
    "axes[1].set_xlim([-74.27, -73.69]) \n",
    "axes[1].set_ylim([40.49, 40.92])\n",
    "axes[1].set_aspect('equal') \n",
    "axes[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 5. Heatmap (Fatores x Tipos)\n",
    "df_heat = conn.execute(\"\"\"\n",
    "    SELECT \"CONTRIBUTING FACTOR VEHICLE 1\" AS factor, \"VEHICLE TYPE CODE 1\" AS vtype\n",
    "    FROM crashes_clean\n",
    "    WHERE factor IS NOT NULL AND vtype IS NOT NULL AND TRIM(factor)<>'' AND TRIM(vtype)<>''\n",
    "\"\"\").df()\n",
    "\n",
    "top_factors = df_heat['factor'].value_counts().head(10).index\n",
    "top_vtypes = df_heat['vtype'].value_counts().head(10).index\n",
    "pivot = pd.crosstab(df_heat[df_heat['factor'].isin(top_factors)]['factor'], \n",
    "                    df_heat[df_heat['vtype'].isin(top_vtypes)]['vtype'])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(pivot, cmap='Blues', annot=True, fmt='d')\n",
    "plt.title('Heatmap: Causa do Acidente vs Tipo de Veículo (Top 10)'); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f7091",
   "metadata": {},
   "source": [
    "# Parte 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estrutura do Dataset Limpo: {df_cleaned.shape}\")\n",
    "\n",
    "# Lista colunas e seus tipos de dados\n",
    "print(\"\\nTipos de dados:\")\n",
    "print(df_cleaned.dtypes)\n",
    "\n",
    "# Visualização das primeiras linhas\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4a4ff",
   "metadata": {},
   "source": [
    "Reestruturação Necessária\n",
    "\n",
    "#### Análise de Variáveis Compostas\n",
    "O dataset atual possui variáveis que violam princípios tidy:\n",
    "1. **Múltiplos veículos** (VEHICLE TYPE CODE 1-5): formato wide\n",
    "2. **Múltiplos fatores** (CONTRIBUTING FACTOR 1-5): formato wide  \n",
    "3. **Múltiplas contagens** (PERSONS/PEDESTRIANS/CYCLISTS/MOTORISTS INJURED/KILLED): podem ser normalizadas\n",
    "\n",
    "Vamos criar diferentes visões tidy para diferentes análises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformação 1: Veículos (Wide -> Long)\")\n",
    "\n",
    "# Seleção de colunas\n",
    "base_cols = ['COLLISION_ID', 'CRASH DATE', 'CRASH TIME', 'DAY_OF_WEEK', 'BOROUGH', 'LATITUDE', 'LONGITUDE']\n",
    "vehicle_type_cols = [c for c in df_cleaned.columns if 'VEHICLE TYPE CODE' in c]\n",
    "\n",
    "# Melt: Transforma colunas de tipo de veículo em linhas\n",
    "df_vehicles_long = df_cleaned.melt(\n",
    "    id_vars=base_cols,\n",
    "    value_vars=vehicle_type_cols,\n",
    "    var_name='vehicle_number',\n",
    "    value_name='vehicle_type'\n",
    ")\n",
    "\n",
    "# Extração do índice do veículo e limpeza de nulos\n",
    "df_vehicles_long['vehicle_number'] = df_vehicles_long['vehicle_number'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "df_vehicles_long = df_vehicles_long[\n",
    "    df_vehicles_long['vehicle_type'].notna() & \n",
    "    (df_vehicles_long['vehicle_type'] != 'N/A')\n",
    "].sort_values(['COLLISION_ID', 'vehicle_number']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset Tidy (Long): {df_vehicles_long.shape}\")\n",
    "print(f\"Colisões únicas: {df_vehicles_long['COLLISION_ID'].nunique():,}\")\n",
    "print(f\"Total de veículos envolvidos: {len(df_vehicles_long):,}\")\n",
    "\n",
    "df_vehicles_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformação 2: Fatores Contribuintes (Wide -> Long)\")\n",
    "\n",
    "base_cols = ['COLLISION_ID', 'CRASH DATE', 'CRASH TIME', 'DAY_OF_WEEK', 'BOROUGH', 'LATITUDE', 'LONGITUDE']\n",
    "factor_cols = [c for c in df_cleaned.columns if 'CONTRIBUTING FACTOR' in c]\n",
    "\n",
    "df_factors_long = df_cleaned.melt(\n",
    "    id_vars=base_cols,\n",
    "    value_vars=factor_cols,\n",
    "    var_name='vehicle_number',\n",
    "    value_name='contributing_factor'\n",
    ")\n",
    "\n",
    "# Extração do índice do veículo e limpeza\n",
    "df_factors_long['vehicle_number'] = df_factors_long['vehicle_number'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Filtra nulos, 'N/A' e strings vazias\n",
    "df_factors_long = df_factors_long[\n",
    "    df_factors_long['contributing_factor'].notna() & \n",
    "    (df_factors_long['contributing_factor'] != 'N/A') & \n",
    "    (df_factors_long['contributing_factor'].str.strip() != '')\n",
    "].sort_values(['COLLISION_ID', 'vehicle_number']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset Tidy (Long): {df_factors_long.shape}\")\n",
    "print(f\"Colisões únicas: {df_factors_long['COLLISION_ID'].nunique():,}\")\n",
    "print(f\"Fatores únicos identificados: {df_factors_long['contributing_factor'].nunique()}\")\n",
    "\n",
    "df_factors_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformação 3: Vítimas (Wide -> Long)\")\n",
    "\n",
    "base_cols = ['COLLISION_ID', 'CRASH DATE', 'CRASH TIME', 'DAY_OF_WEEK', 'BOROUGH', 'LATITUDE', 'LONGITUDE']\n",
    "casualty_cols = [c for c in df_cleaned.columns if 'NUMBER OF' in c]\n",
    "\n",
    "df_casualties_long = df_cleaned.melt(\n",
    "    id_vars=base_cols,\n",
    "    value_vars=casualty_cols,\n",
    "    var_name='description',\n",
    "    value_name='count'\n",
    ")\n",
    "\n",
    "# Mantém apenas registros onde houve vítimas (count > 0)\n",
    "df_casualties_long = df_casualties_long[df_casualties_long['count'] > 0].copy()\n",
    "\n",
    "# Extrai Tipo e Gravidade usando Regex na descrição da coluna\n",
    "# Ex: \"NUMBER OF PEDESTRIANS INJURED\" -> Grupo 1: PEDESTRIANS, Grupo 2: INJURED\n",
    "pattern = r'NUMBER OF (.+) (INJURED|KILLED)'\n",
    "extracted = df_casualties_long['description'].str.extract(pattern)\n",
    "df_casualties_long['victim_type'] = extracted[0]\n",
    "df_casualties_long['severity'] = extracted[1]\n",
    "\n",
    "# Remove coluna auxiliar\n",
    "df_casualties_long = df_casualties_long.drop(columns=['description'])\n",
    "\n",
    "print(f\"Dataset Tidy (Long): {df_casualties_long.shape}\")\n",
    "print(f\"Total de vítimas contabilizadas: {df_casualties_long['count'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\nDistribuição por tipo:\")\n",
    "print(df_casualties_long.groupby(['victim_type', 'severity'])['count'].sum().sort_values(ascending=False))\n",
    "\n",
    "df_casualties_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformação 4: Tabela Principal Tidy\")\n",
    "\n",
    "# Seleção de colunas atômicas (variáveis únicas por colisão)\n",
    "cols_main = [\n",
    "    'COLLISION_ID', 'CRASH DATE', 'CRASH TIME', 'DAY_OF_WEEK',\n",
    "    'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE',\n",
    "    'ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME',\n",
    "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
    "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
    "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'\n",
    "]\n",
    "\n",
    "df_tidy_main = df_cleaned[cols_main].copy()\n",
    "\n",
    "print(f\"Shape Tabela Principal: {df_tidy_main.shape}\")\n",
    "print(\"\\nTipos de dados:\")\n",
    "print(df_tidy_main.dtypes)\n",
    "\n",
    "# Resumo estatístico\n",
    "df_tidy_main.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e071ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação da estrutura final\n",
    "print(\"Resumo dos Datasets Tidy\")\n",
    "for nome, df in [('Principal', df_tidy_main), ('Veículos', df_vehicles_long), \n",
    "                 ('Fatores', df_factors_long), ('Vítimas', df_casualties_long)]:\n",
    "    print(f\"\\n[{nome}] Shape: {df.shape} | Duplicatas: {df.duplicated().sum()}\")\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registro dos DataFrames no DuckDB\n",
    "conn.register('collisions_tidy', df_tidy_main)\n",
    "conn.register('vehicles_tidy', df_vehicles_long)\n",
    "conn.register('factors_tidy', df_factors_long)\n",
    "conn.register('casualties_tidy', df_casualties_long)\n",
    "\n",
    "# Validação dos registros (Contagens e Somas)\n",
    "queries = {\n",
    "    'collisions_tidy (rows)': \"SELECT COUNT(*) FROM collisions_tidy\",\n",
    "    'vehicles_tidy (rows)': \"SELECT COUNT(*) FROM vehicles_tidy\",\n",
    "    'factors_tidy (rows)': \"SELECT COUNT(*) FROM factors_tidy\",\n",
    "    'casualties_tidy (total victims)': \"SELECT SUM(count) FROM casualties_tidy\"\n",
    "}\n",
    "\n",
    "print(\"Status do Registro no DuckDB\")\n",
    "for label, q in queries.items():\n",
    "    res = conn.execute(q).fetchone()[0]\n",
    "    print(f\"{label}: {res:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "datasets = {\n",
    "    'collisions_tidy.parquet': df_tidy_main,\n",
    "    'vehicles_tidy.parquet': df_vehicles_long,\n",
    "    'factors_tidy.parquet': df_factors_long,\n",
    "    'casualties_tidy.parquet': df_casualties_long,\n",
    "    'df_cleaned_full.parquet': df_cleaned\n",
    "}\n",
    "\n",
    "stats = []\n",
    "for fname, df in datasets.items():\n",
    "    fpath = os.path.join(output_dir, fname)\n",
    "    df.to_parquet(fpath, compression='snappy', index=False)\n",
    "    \n",
    "    stats.append({\n",
    "        'Arquivo': fname,\n",
    "        'Linhas': len(df),\n",
    "        'Colunas': df.shape[1],\n",
    "        'MB': round(os.path.getsize(fpath) / (1024**2), 2)\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats)\n",
    "print(f\"Exportação concluída! Tamanho total: {df_stats['MB'].sum():.2f} MB\\n\")\n",
    "print(df_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Consulta 1: Análise Temporal (Últimos 24 Meses)\")\n",
    "\n",
    "query_temporal = \"\"\"\n",
    "WITH monthly_data AS (\n",
    "    SELECT \n",
    "        STRFTIME(\"CRASH DATE\", '%Y-%m') AS year_month,\n",
    "        COUNT(*) AS collisions,\n",
    "        SUM(\"NUMBER OF PERSONS INJURED\") AS injured,\n",
    "        SUM(\"NUMBER OF PERSONS KILLED\") AS killed\n",
    "    FROM collisions_tidy\n",
    "    WHERE \"CRASH DATE\" IS NOT NULL\n",
    "    GROUP BY year_month\n",
    "),\n",
    "with_trends AS (\n",
    "    SELECT \n",
    "        year_month,\n",
    "        collisions,\n",
    "        injured,\n",
    "        killed,\n",
    "        -- Média Móvel de 3 meses para suavizar tendências\n",
    "        AVG(collisions) OVER (\n",
    "            ORDER BY year_month \n",
    "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "        ) AS moving_avg_3m,\n",
    "        -- Comparativo percentual com o mês anterior\n",
    "        ROUND(100.0 * (collisions - LAG(collisions) OVER (ORDER BY year_month)) / \n",
    "              NULLIF(LAG(collisions) OVER (ORDER BY year_month), 0), 2) AS mom_growth_pct\n",
    "    FROM monthly_data\n",
    ")\n",
    "SELECT * FROM with_trends ORDER BY year_month DESC LIMIT 24;\n",
    "\"\"\"\n",
    "\n",
    "df_temporal = conn.execute(query_temporal).df()\n",
    "\n",
    "# Visualização Unificada: Evolução de Colisões\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Dados reais\n",
    "plt.plot(df_temporal['year_month'], df_temporal['collisions'], \n",
    "         marker='o', linestyle='-', color='#1f77b4', label='Colisões Mensais', alpha=0.8)\n",
    "\n",
    "# Tendência (Média Móvel)\n",
    "plt.plot(df_temporal['year_month'], df_temporal['moving_avg_3m'], \n",
    "         color='red', linestyle='--', linewidth=2, label='Média Móvel (3 Meses)')\n",
    "\n",
    "plt.title('Tendência Temporal de Colisões (Últimos 2 Anos)')\n",
    "plt.ylabel('Volume de Colisões')\n",
    "plt.xlabel('Mês de Referência')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle=':', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de44dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Consulta 2: Perfil de Vítimas por bairro\")\n",
    "\n",
    "query_borough = \"\"\"\n",
    "SELECT \n",
    "    COALESCE(BOROUGH, 'Unknown') AS borough,\n",
    "    COUNT(*) AS total_collisions,\n",
    "    SUM(\"NUMBER OF PEDESTRIANS INJURED\") AS ped_injured,\n",
    "    SUM(\"NUMBER OF CYCLIST INJURED\") AS cyc_injured,\n",
    "    SUM(\"NUMBER OF MOTORIST INJURED\") AS mot_injured,\n",
    "    -- Taxa de severidade (feridos por colisão)\n",
    "    ROUND(AVG(\"NUMBER OF PERSONS INJURED\"), 3) AS avg_severity\n",
    "FROM collisions_tidy\n",
    "GROUP BY borough\n",
    "ORDER BY total_collisions DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_borough = conn.execute(query_borough).df()\n",
    "\n",
    "print(df_borough.to_string(index=False))\n",
    "\n",
    "# Prepara dados para o gráfico (Melt para formato longo compatível com Seaborn)\n",
    "df_melt = df_borough.melt(\n",
    "    id_vars='borough', \n",
    "    value_vars=['ped_injured', 'cyc_injured', 'mot_injured'],\n",
    "    var_name='Tipo Vítima', \n",
    "    value_name='Qtd Feridos'\n",
    ")\n",
    "\n",
    "df_melt['Tipo Vítima'] = df_melt['Tipo Vítima'].replace({\n",
    "    'ped_injured': 'Pedestres', \n",
    "    'cyc_injured': 'Ciclistas', \n",
    "    'mot_injured': 'Motoristas'\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_melt, x='borough', y='Qtd Feridos', hue='Tipo Vítima', palette='viridis')\n",
    "\n",
    "plt.title('Perfil de Vítimas por Bairro: Quem se fere mais?')\n",
    "plt.ylabel('Total de Feridos')\n",
    "plt.xlabel('Bairro')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.legend(title='Tipo de Vítima')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141022b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Consulta 3: Matriz de Risco (Frequência vs Severidade)\")\n",
    "\n",
    "# Query simplificada para métricas de risco\n",
    "query_factors = \"\"\"\n",
    "SELECT \n",
    "    contributing_factor AS factor,\n",
    "    COUNT(DISTINCT c.COLLISION_ID) AS frequency,\n",
    "    -- Índice de Severidade: (Feridos + 10x Mortos) / Total Colisões\n",
    "    ROUND(SUM(c.\"NUMBER OF PERSONS INJURED\" + (c.\"NUMBER OF PERSONS KILLED\" * 10)) * 1.0 / \n",
    "          COUNT(DISTINCT c.COLLISION_ID), 2) AS severity_index,\n",
    "    SUM(c.\"NUMBER OF PERSONS KILLED\") AS total_killed\n",
    "FROM factors_tidy f\n",
    "JOIN collisions_tidy c ON f.COLLISION_ID = c.COLLISION_ID\n",
    "WHERE f.contributing_factor != 'Unspecified'\n",
    "GROUP BY factor\n",
    "HAVING frequency > 100 -- Filtro de relevância estatística\n",
    "ORDER BY frequency DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_risk = conn.execute(query_factors).df()\n",
    "\n",
    "print(f\"Fatores analisados: {len(df_risk)}\")\n",
    "print(df_risk.head(10).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# X: Frequência (Log), Y: Severidade, Cor/Tamanho: Total de Mortos\n",
    "scatter = plt.scatter(\n",
    "    df_risk['frequency'], \n",
    "    df_risk['severity_index'], \n",
    "    s=df_risk['total_killed'] * 5 + 50, #\n",
    "    c=df_risk['severity_index'],        \n",
    "    cmap='RdYlGn_r',                  \n",
    "    alpha=0.7, \n",
    "    edgecolors='grey'\n",
    ")\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title('Matriz de Risco: Frequência vs. Severidade dos Fatores')\n",
    "plt.xlabel('Frequência de Ocorrência (Escala Log)')\n",
    "plt.ylabel('Índice de Severidade (Impacto por Acidente)')\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.3)\n",
    "\n",
    "\n",
    "for _, row in df_risk.nlargest(3, 'frequency').iterrows():\n",
    "    plt.text(row['frequency'], row['severity_index'], row['factor'][:20], \n",
    "             fontsize=9, ha='right', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "for _, row in df_risk.nlargest(3, 'severity_index').iterrows():\n",
    "    plt.text(row['frequency'], row['severity_index'], row['factor'][:20], \n",
    "             fontsize=9, ha='left', va='top', color='red', fontweight='bold')\n",
    "\n",
    "plt.colorbar(scatter, label='Índice de Severidade')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Consulta 4: Padrões Temporais (Heatmap Semanal)\")\n",
    "\n",
    "query_time = \"\"\"\n",
    "SELECT \n",
    "    \"DAY_OF_WEEK\" AS day_of_week,\n",
    "    EXTRACT(HOUR FROM \"CRASH TIME\") AS hour,\n",
    "    COUNT(*) AS collisions\n",
    "FROM collisions_tidy\n",
    "WHERE \"CRASH TIME\" IS NOT NULL\n",
    "GROUP BY day_of_week, hour;\n",
    "\"\"\"\n",
    "\n",
    "df_time = conn.execute(query_time).df()\n",
    "\n",
    "# Tradução e Ordenação\n",
    "day_map = {\n",
    "    'Sunday': 'Domingo', 'Monday': 'Segunda', 'Tuesday': 'Terça',\n",
    "    'Wednesday': 'Quarta', 'Thursday': 'Quinta', 'Friday': 'Sexta', 'Saturday': 'Sábado'\n",
    "}\n",
    "day_order = ['Domingo', 'Segunda', 'Terça', 'Quarta', 'Quinta', 'Sexta', 'Sábado']\n",
    "\n",
    "# Cria a matriz (Pivot Table): Linhas = Horas, Colunas = Dias\n",
    "pivot_table = df_time.pivot_table(\n",
    "    index='hour', \n",
    "    columns='day_of_week', \n",
    "    values='collisions', \n",
    "    aggfunc='sum'\n",
    ").rename(columns=day_map).reindex(columns=day_order)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_table, \n",
    "    cmap='YlOrRd',      \n",
    "    annot=True,         \n",
    "    fmt='.0f',         \n",
    "    linewidths=.5,     \n",
    "    cbar_kws={'label': 'Volume de Colisões'}\n",
    ")\n",
    "\n",
    "plt.title('Mapa de Calor: Frequência de Acidentes (Dia x Hora)')\n",
    "plt.ylabel('Hora do Dia')\n",
    "plt.xlabel('Dia da Semana')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e0680",
   "metadata": {},
   "source": [
    "### teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8d6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta 6: Mapa Simples de Hotspots\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConsulta 6: Mapa Simples de Hotspots\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m query_hotspots = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mSELECT \u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m    ROUND(LATITUDE, 3) AS lat,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33mLIMIT 100;\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df_hotspots = \u001b[43mconn\u001b[49m.execute(query_hotspots).df()\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_hotspots.head().to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m     21\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m8\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Consulta 6: Mapa Simples de Hotspots\")\n",
    "\n",
    "query_hotspots = \"\"\"\n",
    "SELECT \n",
    "    ROUND(LATITUDE, 3) AS lat,\n",
    "    ROUND(LONGITUDE, 3) AS lon,\n",
    "    COALESCE(BOROUGH, 'Desconhecido') AS borough,\n",
    "    COUNT(*) AS collisions\n",
    "FROM collisions_tidy\n",
    "WHERE LATITUDE BETWEEN 40.49 AND 40.92 \n",
    "  AND LONGITUDE BETWEEN -74.26 AND -73.69\n",
    "GROUP BY lat, lon, borough\n",
    "HAVING collisions >= 10\n",
    "ORDER BY collisions DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "df_hotspots = conn.execute(query_hotspots).df()\n",
    "print(df_hotspots.head().to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_hotspots, \n",
    "    x='lon', \n",
    "    y='lat', \n",
    "    hue='borough',  # Colore por região para facilitar a leitura\n",
    "    palette='bright',\n",
    "    s=60,           # Tamanho fixo dos pontos\n",
    "    alpha=0.7       # Leve transparência\n",
    ")\n",
    "\n",
    "plt.title('Distribuição Geográfica dos 100 Maiores Pontos de Acidente')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, linestyle=':', alpha=0.5)\n",
    "plt.legend(title='Região', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b827881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Derivação de Variáveis Auxiliares\")\n",
    "\n",
    "# 1. Padronização de Tempo\n",
    "# Garante que CRASH TIME é string para extração segura, depois converte para data\n",
    "time_objs = pd.to_datetime(df_tidy_main['CRASH TIME'].astype(str), format='%H:%M:%S', errors='coerce')\n",
    "df_tidy_main['hour'] = time_objs.dt.hour.fillna(0).astype(int) # Preenche nulos com 0 por segurança\n",
    "\n",
    "# 2. Variáveis de Calendário\n",
    "df_tidy_main['day_of_week'] = df_tidy_main['DAY_OF_WEEK']\n",
    "df_tidy_main['is_weekend'] = df_tidy_main['day_of_week'].isin(['Saturday', 'Sunday'])\n",
    "\n",
    "# 3. Classificação de Período (Vetorizada com np.select)\n",
    "cond_night = df_tidy_main['hour'].isin([22, 23, 0, 1, 2, 3, 4, 5])\n",
    "cond_peak = df_tidy_main['hour'].isin([7, 8, 9, 17, 18, 19])\n",
    "\n",
    "df_tidy_main['hour_period'] = np.select(\n",
    "    [cond_night, cond_peak], \n",
    "    ['Noite/Madrugada', 'Pico'], \n",
    "    default='Intermediário'\n",
    ")\n",
    "\n",
    "# 4. Métricas de Severidade\n",
    "# Colunas totais diretamente para evitar dupla contagem.\n",
    "df_tidy_main['fatalities_total'] = df_tidy_main['NUMBER OF PERSONS KILLED'].fillna(0)\n",
    "df_tidy_main['injuries_total'] = df_tidy_main['NUMBER OF PERSONS INJURED'].fillna(0)\n",
    "\n",
    "# Índice de Severidade (Peso 5 para mortes)\n",
    "df_tidy_main['severity_index'] = df_tidy_main['injuries_total'] + (df_tidy_main['fatalities_total'] * 5)\n",
    "\n",
    "cols_view = ['day_of_week', 'is_weekend', 'hour', 'hour_period', 'fatalities_total', 'severity_index']\n",
    "print(df_tidy_main[cols_view].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65991756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Análise Univariada\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "ax = axes.flatten()\n",
    "\n",
    "# 1. Horário \n",
    "sns.countplot(data=df_tidy_main, x='hour', ax=ax[0], color='#4c72b0')\n",
    "ax[0].set_title('Distribuição de Acidentes por Hora')\n",
    "\n",
    "# 2. Período do Dia\n",
    "sns.countplot(data=df_tidy_main, x='hour_period', ax=ax[1], palette='Set2')\n",
    "ax[1].set_title('Concentração por Período')\n",
    "\n",
    "# 3. Dia da Semana\n",
    "order_days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "sns.countplot(data=df_tidy_main, x='day_of_week', order=order_days, ax=ax[2], palette='viridis')\n",
    "ax[2].tick_params(axis='x', rotation=45)\n",
    "ax[2].set_title('Distribuição Semanal')\n",
    "\n",
    "# 4. Distribuições de Severidade e Vítimas\n",
    "# Nota: Escala Log no eixo Y porque a distribuição é muito assimétrica (muitos zeros)\n",
    "vars_hist = [\n",
    "    ('severity_index', 'Índice de Severidade', '#dd8452'),\n",
    "    ('fatalities_total', 'Fatalidades Totais', '#55a868'),\n",
    "    ('injuries_total', 'Feridos Totais', '#c44e52')\n",
    "]\n",
    "\n",
    "for i, (col, title, color) in enumerate(vars_hist):\n",
    "    idx = i + 3\n",
    "    sns.histplot(df_tidy_main[col], ax=ax[idx], bins=20, color=color, kde=False)\n",
    "    ax[idx].set_yscale('log')  # Essencial para visualizar caudas longas\n",
    "    ax[idx].set_title(f'{title} (Escala Log)')\n",
    "    ax[idx].set_ylabel('Frequência (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estatísticas Consolidadas\n",
    "cols_stats = ['fatalities_total', 'injuries_total', 'severity_index']\n",
    "\n",
    "# Calcula estatísticas descritivas\n",
    "desc = df_tidy_main[cols_stats].describe().T\n",
    "\n",
    "# Adiciona Skew e Kurtosis\n",
    "desc['skew'] = df_tidy_main[cols_stats].skew()\n",
    "desc['kurtosis'] = df_tidy_main[cols_stats].kurt()\n",
    "\n",
    "# Calcula Outliers\n",
    "Q1 = df_tidy_main[cols_stats].quantile(0.25)\n",
    "Q3 = df_tidy_main[cols_stats].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_mask = (df_tidy_main[cols_stats] < (Q1 - 1.5 * IQR)) | (df_tidy_main[cols_stats] > (Q3 + 1.5 * IQR))\n",
    "desc['outliers_count'] = outlier_mask.sum()\n",
    "\n",
    "print(\"\\nResumo Estatístico e Distribuição:\")\n",
    "print(desc[['mean', 'std', 'max', 'skew', 'kurtosis', 'outliers_count']].round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Análise Bivariada e Correlações\")\n",
    "\n",
    "# 1. Matriz de Correlação\n",
    "cols_corr = ['fatalities_total', 'injuries_total', 'severity_index', 'hour']\n",
    "corr = df_tidy_main[cols_corr].corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlação Spearman')\n",
    "plt.show()\n",
    "\n",
    "# 2. Severidade por hora\n",
    "hour_stats = df_tidy_main.groupby('hour')[['severity_index', 'injuries_total', 'fatalities_total']].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.lineplot(data=hour_stats, x='hour', y='severity_index', marker='o', color='#c44e52')\n",
    "plt.title('Severidade Média por Hora do Dia')\n",
    "plt.ylabel('Índice Médio de Severidade')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.show()\n",
    "\n",
    "# 3. Severidade por bairro\n",
    "borough_stats = df_tidy_main.groupby('BOROUGH').agg({\n",
    "    'severity_index': 'mean',\n",
    "    'fatalities_total': 'mean',\n",
    "    'injuries_total': 'mean',\n",
    "    'COLLISION_ID': 'count'\n",
    "}).rename(columns={'COLLISION_ID': 'total_collisions'}).reset_index()\n",
    "\n",
    "borough_stats = borough_stats.sort_values('severity_index', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=borough_stats, x='BOROUGH', y='severity_index', palette='magma')\n",
    "plt.title('Severidade Média por Bairro (Borough)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Índice Médio de Severidade')\n",
    "plt.show()\n",
    "\n",
    "print(\"Estatísticas por Borough:\")\n",
    "print(borough_stats.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f70663",
   "metadata": {},
   "source": [
    "## TESTE DE HIPOTESES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "print(\"Teste de Hipóteses\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# H1: Letalidade Noturna (22h-05h)\n",
    "df_tidy_main['is_night'] = df_tidy_main['hour'].isin([22, 23, 0, 1, 2, 3, 4, 5])\n",
    "df_tidy_main['has_fatality'] = df_tidy_main['fatalities_total'] > 0\n",
    "\n",
    "# Teste Chi-Quadrado\n",
    "tab_h1 = pd.crosstab(df_tidy_main['is_night'], df_tidy_main['has_fatality'])\n",
    "chi2, p_val_h1, _, _ = chi2_contingency(tab_h1)\n",
    "\n",
    "rate_night = tab_h1.loc[True, True] / tab_h1.loc[True].sum()\n",
    "rate_day = tab_h1.loc[False, True] / tab_h1.loc[False].sum()\n",
    "\n",
    "accepted_h1 = (p_val_h1 < 0.05) and (rate_night > rate_day)\n",
    "\n",
    "results.append({\n",
    "    'Hipótese': 'H1: Maior letalidade noturna',\n",
    "    'Taxa A (Noite)': f\"{rate_night*100:.3f}%\",\n",
    "    'Taxa B (Dia)': f\"{rate_day*100:.3f}%\",\n",
    "    'P-Valor': p_val_h1,\n",
    "    'Resultado': 'ACEITA' if accepted_h1 else 'REJEITADA'\n",
    "})\n",
    "\n",
    "# H2: Perfil Horário de Pico (07-09h, 17-19h)\n",
    "peak_hours = [7, 8, 9, 17, 18, 19]\n",
    "df_tidy_main['is_peak'] = df_tidy_main['hour'].isin(peak_hours)\n",
    "\n",
    "h2_stats = df_tidy_main.groupby('is_peak')[['injuries_total', 'fatalities_total']].mean()\n",
    "\n",
    "injuries_peak = h2_stats.loc[True, 'injuries_total']\n",
    "injuries_off = h2_stats.loc[False, 'injuries_total']\n",
    "fatal_peak = h2_stats.loc[True, 'fatalities_total']\n",
    "fatal_off = h2_stats.loc[False, 'fatalities_total']\n",
    "\n",
    "# Critério: Pico deve ter MAIS feridos e MENOS mortes (acidentes leves)\n",
    "accepted_h2 = (injuries_peak > injuries_off) and (fatal_peak < fatal_off)\n",
    "\n",
    "results.append({\n",
    "    'Hipótese': 'H2: Pico (+Feridos / -Mortos)',\n",
    "    'Taxa A (Pico)': f\"Inj:{injuries_peak:.3f} | Fat:{fatal_peak:.4f}\",\n",
    "    'Taxa B (Fora)': f\"Inj:{injuries_off:.3f} | Fat:{fatal_off:.4f}\",\n",
    "    'P-Valor': 'N/A',\n",
    "    'Resultado': 'ACEITA' if accepted_h2 else 'REJEITADA'\n",
    "})\n",
    "\n",
    "# Resultado Final\n",
    "print(pd.DataFrame(results).to_string(index=False))\n",
    "\n",
    "if accepted_h1:\n",
    "    print(f\"\\n[H1] A noite é {rate_night/rate_day:.1f}x mais letal que o dia.\")\n",
    "if accepted_h2:\n",
    "    print(\"[H2] Horários de pico têm maior frequência de feridos, mas menor letalidade.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
